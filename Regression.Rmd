---
title: "Linear Regression Analysis"
output: pdf_document
date: "2025-03-06"
---
```
install.packages("ISLR2")
library(ISLR2)
```

```{r setup, include=FALSE}
library(MASS)
library(ISLR2)
library(car)
```

```{r}
uniqId <- unlist(lapply(totalLabels$col1, function(x) unlist(strsplit(x, "_"))[[1]]))
z_score$uniqID <- uniqId
```

```{r}
install.packages("dplyr")  # Install dplyr if not installed
library(dplyr)  # Load dplyr
install.packages("keras")
library(keras)
library(ggplot2)
```
```{r}

# Sample dataset
set.seed(123)

z_score["pma"] <- annotation$PMA
tmp <- z_score[z_score$category == 1,]
tmp <- table(tmp$uniqID)
tmp <- names(tmp[tmp >= 1]) # 67 id has 1s, total 110

idSet <- tmp
data <- z_score[z_score$uniqID %in% idSet,]

```
```{r}
data <- data.frame(
  sti = data$STI,
  nns = data$NNS.Cycles,
  pma = data$pma,
  category = as.factor(data$category),
  id = data$uniqID
)
```

```{r}
# Fit Linear Regression Model
model <- lm(nns ~ pma, data = data)
model <- lm(nns ~ pma + I(pma^2), data = data)

group_models <- data %>%
  group_by(id) %>%
  do(model = lm(nns ~ pma + I(pma^2), data = .))  # Fit regression per group


ggplot(data, aes(x = pma, y = nns, color = category)) +
  geom_point() +
  #geom_smooth(method = "lm", se = FALSE) +  # Regression for each group
  geom_smooth(se = FALSE, method = "lm", aes(group = id), col = "black", linetype = "dashed") +  # Overall regression
  labs(title = "Linear Regression: STI vs PMA") +
  theme_minimal()


geom_smooth(se = FALSE, method = "lm", formula = y ~ poly(x, 2), aes(group = id), col = "black", linetype = "dashed")

ggplot(data, aes(x = pma, y = nns, color = category)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", formula = y ~ poly(x, 2), aes(group = id), col = "black", linetype = "dashed") +
  labs(title = "Quadratic Regression: STI vs PMA") +
  theme_minimal()


```

```{r}

# View model summary for first group
summary(group_models$model[[1]])

# Plot Regression Fit
ggplot(data, aes(x = pma, y = nns, color = factor(category))) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Linear Regression: Recovery Time vs Age", color = "Category")+
  theme_classic()

```
âœ… 1. Predicting Positive/Negative (Classification) over Time
If you're trying to predict whether a time point will be "positive" or "negative", given historical trends and values:

ðŸ”® Best Methods:
Method	Why It's Good
Logistic Regression with Lag Features	Simple, interpretable. Add previous time point data as features.
Random Forest / Gradient Boosting (e.g. XGBoost)	Handles non-linearity, interactions, and time patterns (if engineered right).
Recurrent Neural Networks (RNNs / LSTM)	Best for long-term dependencies in sequential data. Good for deep time series.
Temporal Convolutional Networks (TCN)	Alternative to RNNs, good for fixed-length prediction windows.

```{r}
# Install if needed
install.packages("xgboost")
install.packages("dplyr")
# install.packages("caret")

library(xgboost)
library(dplyr)
library(caret)

# -------------------------------
# 1. Simulate dataset
# -------------------------------
set.seed(42)
n_subjects <- 67
time_points <- 20

data_list <- lapply(1:n_subjects, function(id) {
  data.frame(
    subjectID = id,
    time = 1:time_points,
    feature1 = runif(time_points),
    feature2 = runif(time_points),
    label = sample(c(0, 1, NA), time_points, replace = TRUE, prob = c(0.4, 0.4, 0.2))
  )
})
df <- bind_rows(data_list)

# -------------------------------
# 2. Drop missing labels
# -------------------------------
df <- df %>% filter(!is.na(label))
df$label <- as.numeric(df$label)

# -------------------------------
# 3. Prepare matrix for XGBoost
# -------------------------------
X <- df %>% select(time, feature1, feature2)
y <- df$label
group <- df$subjectID

# XGBoost needs numeric matrix
X_mat <- as.matrix(X)

# -------------------------------
# 4. Custom grouped CV by subject
# -------------------------------
folds <- groupKFold(group, k = 5)  # From caret

for (i in seq_along(folds)) {
  cat("\nðŸ§ª Fold", i, "\n")
  
  train_idx <- folds[[i]]
  test_idx <- setdiff(seq_len(nrow(df)), train_idx)
  
  dtrain <- xgb.DMatrix(data = X_mat[train_idx, ], label = y[train_idx])
  dtest <- xgb.DMatrix(data = X_mat[test_idx, ], label = y[test_idx])
  
  model <- xgb.train(
    data = dtrain,
    objective = "binary:logistic",
    nrounds = 50,
    verbose = 0
  )
  
  preds <- predict(model, dtest)
  pred_class <- ifelse(preds > 0.5, 1, 0)
  
  confusion <- confusionMatrix(factor(pred_class), factor(y[test_idx]))
  print(confusion)
}


```

LSTM: 
Good for long-sequence medical time series.
âœ” Handles missing timestamps better than traditional models.
âœ– Can be slow to train.
```{r}
# Install required packages


# Define LSTM Model
model <- keras_model_sequential() %>%
  layer_lstm(units = 50, return_sequences = TRUE, input_shape = c(time_steps, features)) %>%
  layer_lstm(units = 50) %>%
  layer_dense(units = 1, activation = "linear") # Regression task

# Compile the model
model %>% compile(
  optimizer = "adam",
  loss = "mse",
  metrics = c("mae")
)

# Fit the model
model %>% fit(x_train, y_train, epochs = 50, batch_size = 32, validation_data = list(x_val, y_val))

```

```{r}
# Install & Load glmnet
install.packages("glmnet")
library(glmnet)

# Prepare data
X <- model.matrix(sti ~ pma + nns, data)[,-1]  # Convert to matrix
Y <- data$sti

# Fit Ridge Regression (alpha = 0 for Ridge, 1 for Lasso)
ridge_model <- glmnet(X, Y, alpha = 0)  # Ridge
lasso_model <- glmnet(X, Y, alpha = 1)  # Lasso

# Cross-validation to find best lambda
cv_ridge <- cv.glmnet(X, Y, alpha = 0)
cv_lasso <- cv.glmnet(X, Y, alpha = 1)

# Best lambda value
cv_ridge$lambda.min
cv_lasso$lambda.min

ggplot(data, aes(x = pma, y = sti, color = category)) +
  geom_point() +
  #geom_smooth(method = "lm", se = FALSE) +  # Regression for each group
  geom_smooth(se = FALSE, method = "loess", aes(group = id), col = "black", linetype = "dashed") +  # Overall regression
  labs(title = "Linear Regression: STI vs PMA") +
  theme_minimal()

```

```{r}
# Install & Load randomForest
install.packages("randomForest")
library(randomForest)

# Fit Random Forest Model
rf_model <- randomForest(recovery_time ~ age + BMI, data = data, ntree = 100)

# Print Model Summary
print(rf_model)

# Feature Importance
importance(rf_model)

```

```{r}
# Install & Load XGBoost
install.packages("xgboost")
library(xgboost)

# Convert to matrix for XGBoost
X <- as.matrix(data[, c("age", "BMI")])
Y <- data$recovery_time

# Train XGBoost model
xgb_model <- xgboost(data = X, label = Y, nrounds = 100, objective = "reg:squarederror")

# Make Predictions
pred <- predict(xgb_model, X)

# Plot Predictions vs Actual
plot(Y, pred, main="XGBoost: Predicted vs Actual Recovery Time", xlab="Actual", ylab="Predicted")
abline(0,1, col="red")

```




```{r}
# Load Advertising Data from local computer
setwd("C:\\Users\\Asus\\Documents\\UP Files\\UPV Subjects\\Stat 197 (Intro to BI)")
advertising <- read.csv(".\\Advertising.csv")
# Check variable names
names(advertising)
```
```{r}
lm.fit <- lm(category ~ ., data=data[, -c(18,19)])
summary(lm.fit)
```



## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
